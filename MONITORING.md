# üìä Monitoring Performance - PANELia

**Date**: 2025-12-08
**Am√©lioration**: #5 - Monitoring Performance
**Priorit√©**: Haute

---

## üéØ Objectif

Ajouter un syst√®me de monitoring pour tracker les performances du scraping :
- Temps de scraping par chapitre
- Vitesse de t√©l√©chargement
- Taux de succ√®s/√©chec
- Statistiques globales

---

## üì¶ Module metrics.py

### Fonctionnalit√©s

**Tracking automatique** :
- Temps de d√©but/fin de chaque chapitre
- Nombre d'images trouv√©es/t√©l√©charg√©es/trait√©es
- Taille des donn√©es t√©l√©charg√©es
- Erreurs rencontr√©es

**M√©triques calcul√©es** :
- Dur√©e de scraping
- Vitesse de t√©l√©chargement (MB/s)
- Taux de succ√®s (%)
- Statistiques agr√©g√©es

**Export** :
- JSON : M√©triques compl√®tes
- CSV : M√©triques par chapitre

---

## üîß Utilisation

### API Simple

```python
from metrics import get_collector

# R√©cup√©rer le collecteur global
collector = get_collector()

# D√©marrer tracking d'un chapitre
collector.start_chapter(1.0, "https://example.com/chapter/1")

# Mettre √† jour les m√©triques
collector.update_chapter(1.0, images_found=10)
collector.add_download(1.0, bytes_downloaded=1024000, success=True)

# Terminer le tracking
collector.end_chapter(1.0, success=True)

# Obtenir les statistiques
stats = collector.get_stats()
print(stats)

# Exporter
collector.export_json("metrics.json")
collector.export_csv("metrics.csv")
```

### Int√©gration Automatique

Le tracking est **automatique** dans :
- `scraper_engine.py` : Track chaque chapitre
- `http_utils.py` : Track chaque t√©l√©chargement

Aucune configuration n√©cessaire !

---

## üìä M√©triques Collect√©es

### Par Chapitre

```python
{
  "chapter_num": 1.0,
  "url": "https://example.com/chapter/1",
  "duration": 45.2,  # secondes
  "images_found": 10,
  "images_downloaded": 10,
  "images_processed": 10,
  "download_errors": 0,
  "total_bytes": 15728640,  # ~15 MB
  "download_speed_mbps": 0.33,
  "success_rate": 100.0,
  "success": true
}
```

### Globales

```python
{
  "session": {
    "start_time": "2025-12-08T18:00:00",
    "duration_seconds": 180.5,
    "duration_human": "3m 0s"
  },
  "chapters": {
    "attempted": 10,
    "succeeded": 9,
    "failed": 1,
    "success_rate": 90.0,
    "avg_duration_seconds": 18.5
  },
  "images": {
    "found": 100,
    "downloaded": 95,
    "processed": 95,
    "errors": 5
  },
  "performance": {
    "total_bytes_downloaded": 157286400,  # ~150 MB
    "total_mb_downloaded": 150.0,
    "avg_speed_mbps": 0.83,
    "total_scraping_time": 180.5
  }
}
```

---

## üíæ Export

### JSON

```python
collector.export_json("metrics.json")
```

**Contient** : Toutes les m√©triques (session + chapitres)

**Usage** : Analyse d√©taill√©e, graphiques

---

### CSV

```python
collector.export_csv("metrics.csv")
```

**Contient** : M√©triques par chapitre (format tabulaire)

**Usage** : Excel, analyse statistique

**Format** :
```csv
chapter_num,url,duration,images_found,images_downloaded,images_processed,download_errors,total_bytes,download_speed_mbps,success_rate,success
1.0,https://...,45.2,10,10,10,0,15728640,0.33,100.0,True
2.0,https://...,38.1,12,11,11,1,18874368,0.42,91.67,True
```

---

## üîç Exemple Complet

```python
from metrics import get_collector, reset_collector

# R√©initialiser pour une nouvelle session
reset_collector()

# R√©cup√©rer le collecteur
collector = get_collector()

# Scraper 3 chapitres
for chap_num in [1.0, 2.0, 3.0]:
    url = f"https://example.com/chapter/{int(chap_num)}"

    # D√©marrer tracking
    collector.start_chapter(chap_num, url)

    try:
        # Simuler scraping
        images_found = 10
        collector.update_chapter(chap_num, images_found=images_found)

        # Simuler t√©l√©chargements
        for _ in range(images_found):
            collector.add_download(chap_num, 1024000, success=True)

        # Succ√®s
        collector.update_chapter(chap_num, images_processed=images_found)
        collector.end_chapter(chap_num, success=True)

    except Exception as e:
        # √âchec
        collector.end_chapter(chap_num, success=False, error_message=str(e))

# Obtenir stats
stats = collector.get_stats()

# Exporter
collector.export_json("session_metrics.json")
collector.export_csv("chapters_metrics.csv")

# Afficher r√©sum√©
print(f"Chapitres r√©ussis: {stats['chapters']['succeeded']}/{stats['chapters']['attempted']}")
print(f"Taux de succ√®s: {stats['chapters']['success_rate']}%")
print(f"Vitesse moyenne: {stats['performance']['avg_speed_mbps']} MB/s")
print(f"Donn√©es t√©l√©charg√©es: {stats['performance']['total_mb_downloaded']} MB")
```

---

## üìà Avantages

### 1. Visibilit√© Performance
- Identifier les chapitres lents
- D√©tecter les bottlenecks
- Optimiser les param√®tres

### 2. Debugging Facilit√©
- Logs d√©taill√©s avec m√©triques
- Erreurs trac√©es par chapitre
- Export pour analyse

### 3. Statistiques Pr√©cises
- Taux de succ√®s r√©el
- Vitesse de t√©l√©chargement
- Temps total vs temps effectif

### 4. Analyse Historique
- Export JSON/CSV
- Comparaison entre sessions
- Graphiques possibles

---

## üîß Int√©gration

### scraper_engine.py

Tracking automatique dans `_process_single_chapter()` :

```python
# D√©marrage
collector.start_chapter(chap_num, chap_url)

# Mise √† jour progressive
collector.update_chapter(chap_num, images_found=len(image_urls))
collector.update_chapter(chap_num, images_downloaded=len(image_bytes_list))
collector.update_chapter(chap_num, images_processed=saved)

# Fin (succ√®s ou √©chec)
collector.end_chapter(chap_num, success=True)
# ou
collector.end_chapter(chap_num, success=False, error_message=str(e))
```

### http_utils.py

Tracking des t√©l√©chargements individuels :

```python
# Succ√®s
collector.add_download(chapter_num, len(img_bytes), success=True)

# √âchec
collector.add_download(chapter_num, 0, success=False)
```

---

## üß™ Tests

### Test Basique

```bash
python -c "
from metrics import MetricsCollector
import time

collector = MetricsCollector()
collector.start_chapter(1.0, 'https://example.com/chapter/1')
collector.update_chapter(1.0, images_found=10)
time.sleep(0.1)
collector.add_download(1.0, 1024000, success=True)
collector.end_chapter(1.0, success=True)

stats = collector.get_stats()
print('Duration:', stats['chapter_details'][0]['duration'], 's')
print('Speed:', stats['chapter_details'][0]['download_speed_mbps'], 'MB/s')
"
```

**R√©sultat attendu** :
```
Duration: 0.1 s
Speed: 9.77 MB/s
```

---

## üìä Prochaines √âtapes (Optionnel)

### Dashboard Streamlit
Ajouter un onglet "M√©triques" dans app.py pour afficher :
- Graphique temps par chapitre
- Graphique vitesse t√©l√©chargement
- Taux de succ√®s
- Statistiques globales

### Alertes
- Alerter si vitesse < seuil
- Alerter si taux d'√©chec > X%
- Notifier fin de batch

### Historique
- Sauvegarder m√©triques dans DB
- Comparer sessions
- Tendances

---

## üõ†Ô∏è Troubleshooting

### M√©triques ne s'affichent pas
```python
from metrics import get_collector

collector = get_collector()
stats = collector.get_stats()

if stats['chapters']['attempted'] == 0:
    print("Aucun chapitre track√©")
else:
    print(f"{stats['chapters']['attempted']} chapitres track√©s")
```

### Reset m√©triques
```python
from metrics import reset_collector

reset_collector()  # Nouvelle session
```

### V√©rifier tracking
```python
collector = get_collector()
print(f"Chapitres en cours: {list(collector.chapters.keys())}")
```

---

## ‚úÖ Checklist Migration

- [x] Cr√©er module metrics.py
- [x] Int√©grer dans scraper_engine.py
- [x] Int√©grer dans http_utils.py
- [x] Tester le tracking basique
- [x] Documentation cr√©√©e
- [ ] Dashboard Streamlit (optionnel)
- [ ] Alertes (optionnel)
- [ ] Historique (optionnel)

---

## üìù Changelog

**v1.0 - 2025-12-08** :
- Module metrics.py cr√©√©
- Int√©gration scraper_engine.py
- Int√©gration http_utils.py
- Export JSON/CSV
- Documentation

---

**Date** : 2025-12-08
**Version** : v1.0
**Statut** : ‚úÖ Fonctionnel
**Am√©lioration** : #5 - Monitoring Performance

üéâ **Syst√®me de monitoring op√©rationnel !**
